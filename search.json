[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Licence",
    "section": "",
    "text": "The text in this guide is released under a Creative Commons Attribution-ShareAlike 4.0 International License. Specific scripts or programming routines may have different licences and authors. If you use the material in any way, make sure to check the Licence note associated to each section and cite it appropriately."
  },
  {
    "objectID": "content/gsi/02-convencionals.html",
    "href": "content/gsi/02-convencionals.html",
    "title": "Assimilating Conventional Observations",
    "section": "",
    "text": "Conventional observations are assimilated from PREPBUFR files. NCEP ADP Global Upper Air and Surface Weather Observations (PREPBUFR format) are composed of a global set of surface and upper air reports operationally collected by the National Centers for Environmental Prediction (NCEP). These include land surface, marine surface, radiosonde, pibal and aircraft reports from the Global Telecommunications System (GTS), profiler and US radar derived winds, SSM/I oceanic winds and TCW retrievals, and satellite wind data from the National Environmental Satellite Data and Information Service (NESDIS). The reports can include pressure, geopotential height, temperature, dew point temperature, wind direction and speed [@cisl_rda_ds337.0].\nWhile the PREPBUFR includes wind derived from satellite observations, GSI ignores this observations and uses the ones provided by the specific bufr file gdas.t00z.satwnd.tm00.bufr_d.\nPREPBUFR files usually contains observations from a 6 to 12 h window and can be modify using FORTRAN routines provided with the GSI code (see util/bufr_tools in the GSI source code folder). You can also create your own bufr file or add new observation to an existing bufr file (see Working with bufr files).\n\nControlling which observations are assimilated\nThe assimilation of conventional observations is controlled with the convinfo file. Let’s check the global_convinfo.txt file we get as an example:\n! otype   = observation type (a7, t, uv, q, etc.)\n! type    = prepbufr observation type (if available)\n! sub     = prepbufr subtype (not yet available)\n! iuse    = flag if to use/not use / monitor data\n!         = 1  use data\n!         = 0  do not use data\n!         = -1 monitor data\n! twindow = time window (+/- hours)\n! numgrp  = cross validation parameter - number of groups\n! ngroup  = cross validation parameter - group to remove from data use\n! nmiter  = cross validation parameter - external iteration to introduce removed data\n! gross   = gross error parameter - gross error\n! ermax   = gross error parameter - max error\n! ermin   = gross error parameter - min error\n! var_b   = variational quality control parameter -  b parameter\n! var_pg ithin rmesh npred  = variational quality control parameter -  pg parameter\n! pmot: the optione to keep thinned datai as monitored, 0: not to keep, other values: to keep\n! ptime: time interval for thinning, 0, no temporal thinning, other values define time interval (less than 6)\n!otype   type  sub iuse twindow numgrp ngroup nmiter gross ermax ermin var_b    var_pg ithin rmesh  pmesh  npred  pmot  ptime\n tcp      112    0    1     3.0      0      0      0  75.0   5.0   1.0  75.0  0.000000     0    0.     0.      0    0.     0.\n ps       120    0    1     3.0      0      0      0   4.0   3.0   1.0   4.0  0.000300     0    0.     0.      0    0.     0.\n ps       132    0   -1     3.0      0      0      0   4.0   3.0   1.0   4.0  0.000300     0    0.     0.      0    0.     0.\n ps       180    0    1     3.0      0      0      0   4.0   3.0   1.0   4.0  0.000300     0    0.     0.      0    0.     0.\n ps       180    01   1     3.0      0      0      0   4.0   3.0   1.0   4.0  0.000300     0    0.     0.      0    0.     0.\nThe head of the file explains the content of each column but there are a few more things to add:\n\ntype: this is defined by the bufr tables, particular Table 2. It is worth checking this table as includes information about which observations are assimilated in GFS, errors asociated to specific instruments and other details.\ntwindow: while the assimilation window is defined in the gsi namalist, it is possible to control an assimilation window for specific observations. This is useful if, for example the assimilation window is 3 h and you want to assimilate temperature in a 1h window.\n\nIn general you only change the isue column to assimilate or not a type of observation and maybe just maybe the gross, ermax, and ermin parameters if you want to modify the quality control of the observations.\n\n\nObservation errors and quality control\nFor regional assimilation GSI uses an error table located in the errtable file that you’ll find in the ./fix folder under the name prepobs_errtable.global (it is confusing that the table for regional errors in in a file called global). Here is a small example of the content of the file for observations from surface stations.\n181 OBSERVATION TYPE\n  0.11000E+04 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.10500E+04 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.10000E+04 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.95000E+03 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.90000E+03 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.85000E+03 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.80000E+03 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.75000E+03 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\n  0.70000E+03 0.15000E+01 0.20000E+01 0.10000E+10 0.16000E+01 0.10000E+10\nThe meaning of each column is described in the following table:\n\n\n\n\n\n\n\n\n\n\n\n\nColumn #\n1\n2\n3\n4\n5\n6\n\n\n\n\nContent\nPressure\nT\nRH\nUV\nPs\nPw\n\n\nUnit\nhPa\ndegreeC\npercent/10\nm/s\nmb\nkg/m2 (or mm)\n\n\n\nSo column 1 define the pressure level associated to the errors for each variable in columns 2 to 6. In the errtable a 0.10000E+10 is a NA.\nGSI will perform a quality control for each observation. In general terms this involves a gross check and specific controls depending on the type of observation.\nFor the gross check, GSI first calculates a ratio:\n\\[ ratio = (obs - bk)/max(ermin, min(ermax, obserror)) \\] The main error parameters are controlled by the convinfo file. The obserror is the observation error defined in the prepbufr file for each observation as a result to the quality control perform while generating that file.\nIf \\(ration &gt; gross\\) the observation is rejected.\nOther piece of information used during the quality control is the quality control flag that is included in the prepbufr file a part if it quality control process. The possible values for conventional observations are:\n\n\n\n\n\n\n\nqc flag\nmeaning\n\n\n\n\n0-2\n–&gt; Obs is assimilated\n\n\n3\nSuspicious obs –&gt; gross check is more strict\n\n\n4-15\n–&gt; Obs is rejected (for some cases 9 o 15 means that the obs is monitored\n\n\n\nYou can find more details about the quality control flags in Table 7.\nGSI can also perform a thinning for conventional observations. You can activate that option for each type of observation changing ithin = 1 in the convinfo file.\nFor each observation GSI will check different things and change the observation error accordingly. The final observation error is recorded in the diag file and then used during the assimilation."
  },
  {
    "objectID": "content/gsi/05-tutorial.html",
    "href": "content/gsi/05-tutorial.html",
    "title": "GSI tutorial",
    "section": "",
    "text": "This tutorial is intended to showcase some of the capabilities of the GSI system. For a more complete set of examples, please visit the official website for GSI: variational methods and [Kalman Filter](https://dtcenter.org/community-code/ensemble-kalman-filter-system-enkf) methods.\nThe tutorial uses the serial Ensemble Square Root Filter (EnSRF) algorithm but could also be used to run the Local Ensemble Kalman Filter (LETKF) algorithm if the GSI system code is compiled using a intel compiler1. It also uses a version of the code that have the capability of assimilating GOES-16 observations."
  },
  {
    "objectID": "content/gsi/05-tutorial.html#moving-parts",
    "href": "content/gsi/05-tutorial.html#moving-parts",
    "title": "GSI tutorial",
    "section": "Moving parts",
    "text": "Moving parts\nThe first component to run the tutorial is the GSI system code that can be found in this repository: github.com/paocorrales/comGSIv3.7_EnKFv1.3. Alternatively, the original code without modifications is available here. The repository includes an example script compile_gsi_yakaira to compile the code. In any case it is important to include the option -DBUILD_WRF=ON to use the ENKF algorithm with WRF.\nThen, clone or download the repository with the scripts, namelists and specific config files from: https://github.com/paocorrales/tutorial_gsi. This repository also includes a bash scripts to download the necessary data to run the tutorial. This data is also available as a Zenodo record.\n\nObservations\nThe observations included in this tutorial are:\nConventional observations: surface and upper air observations (meteorological stations, radiosondes, airplanes, etc.). It also includes wind derived from satellite. The bufr file used here is created from the prepbufr file at 12 UTC of 2018/11/22 and includes also observations from automatic station networks in Argentina and other countries.\n\nFile: cimap.20181122.t12z.01h.prepbufr.nqc\n\nRadiance observations: radiances from polar and geostationary satellites. Observations from polar satellites comes from the Global Data Assimilation System (GDAS) Model: https://www.nco.ncep.noaa.gov/pmb/products/gfs. The GOES-16 observations (ABI sensor) are derived from the public netcdf files published by NOAA.\nFiles:\n\nabig16.20181122.t12z.bufr_d\n1bamua.20181122.t12z.bufr_d\nssmisu.20181122.t12z.bufr_d\n1bhrs4.20181122.t12z.bufr_d\nairsev.20181122.t12z.bufr_d\nmtiasi.20181122.t12z.bufr_d\n1bmhs.20181122.t12z.bufr_d\natms.20181122.t12z.bufr_d\nsatwnd.20181122.t12z.bufr_d\n\n\n\nBackground\nThe background files includes the 10-member ensemble generated using the WRF-ARW numerical model for a regional domain centered in the center and northern Argentina. For more information about the model configuration see https://doi.org/10.1016/j.atmosres.2022.106456\n\nThe 00 subfolder includes a 10-member ensemble and the ensemble mean to run the GSI system using the ENKF version.\nThe 01 to 10 subfolders include the background at the analysis time and files every 10 minutes inside the assimilation window to run the GSI system using the FGAT method"
  },
  {
    "objectID": "content/gsi/05-tutorial.html#structure",
    "href": "content/gsi/05-tutorial.html#structure",
    "title": "GSI tutorial",
    "section": "Structure",
    "text": "Structure\nBy cloning the tutorial repo and downloading the associated data with the provied script you will end up with the following folder structure.\ntutorial_gsi/\n├── download_data.sh\n├── fix\n│   ├── global_satinfo.txt\n│   ├── satbias_ang\n│   ├── satbias_in\n│   └── satbias_pc_in\n├── GUESS\n│   └── 20181122120000\n│       ├── 00\n│       │   ├── wrfarw.ensmean\n│       │   ├── wrfarw.mem001\n│       │   ├── wrfarw.mem002\n│       │   ├── wrfarw.mem003\n│       │   ├── wrfarw.mem004\n│       │   ├── wrfarw.mem005\n│       │   ├── wrfarw.mem006\n│       │   ├── wrfarw.mem007\n│       │   ├── wrfarw.mem008\n│       │   ├── wrfarw.mem009\n│       │   └── wrfarw.mem010\n│       ├── 01\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 02\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 03\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 04\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 05\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 06\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 07\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 08\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       ├── 09\n│       │   ├── wrf_inou1\n│       │   ├── wrf_inou2\n│       │   ├── wrf_inou3\n│       │   ├── wrf_inou4\n│       │   ├── wrf_inou5\n│       │   ├── wrf_inou6\n│       │   └── wrf_inou7\n│       └── 10\n│           ├── wrf_inou1\n│           ├── wrf_inou2\n│           ├── wrf_inou3\n│           ├── wrf_inou4\n│           ├── wrf_inou5\n│           ├── wrf_inou6\n│           └── wrf_inou7\n├── namelists\n│   ├── comenkf_namelist.sh\n│   └── comgsi_namelist.sh\n├── OBS\n│   ├── 1bamua.20181122.t12z.bufr_d\n│   ├── 1bhrs4.20181122.t12z.bufr_d\n│   ├── 1bmhs.20181122.t12z.bufr_d\n│   ├── abig16.20181122.t12z.bufr_d\n│   ├── airsev.20181122.t12z.bufr_d\n│   ├── atms.20181122.t12z.bufr_d\n│   ├── cimap.20181122.t12z.01h.prepbufr.nqc\n│   ├── mtiasi.20181122.t12z.bufr_d\n│   ├── satwnd.20181122.t12z.bufr_d\n│   └── ssmisu.20181122.t12z.bufr_d\n├── README.md\n├── run_enkf.sh\n└── run_gsi.sh\nA GSI folder will be created when running the run_gsi.sh script and a ENKF folder will be created when running the run_enkf.sh that performs the analysis."
  },
  {
    "objectID": "content/gsi/05-tutorial.html#fgat",
    "href": "content/gsi/05-tutorial.html#fgat",
    "title": "GSI tutorial",
    "section": "Running GSI",
    "text": "Running GSI\nAs we focus on running GSI with the Kalman Filter method, the first stem is to run GSI as Observation operator. So, the system will compare the observations with the background state and and save that information in diagnostic files.\nThe example used in this tutorial is relatively small, so while you may need a HPC system for real cases, this one can be run in a small server or even a computer with at least 10 processors.\nHere are the ~20 first lines of the script run_gsi.sh:\n#PBS -N TEST-1-GSI \n#PBS -m abe \n#PBS -l walltime=03:00:00 \n#PBS -l nodes=1:ppn=24 \n#PBS -j oe \n\nBASEDIR=/home/paola.corrales/datosmunin3/tutorial_gsi           # Path to the tutorial folder\nGSIDIR=/home/paola.corrales/datosmunin3/comGSIv3.7_EnKFv1.3     # Path to where the GSI/EnKF code is compiled \nFECHA_INI='11:00:00 2018-11-22'                                 # Init time (analysis time - $ANALISIS)\nANALISIS=3600                                                   # Assimilation cycle in seconds\nOBSWIN=1                                                        # Assimilation window in hours\nN_MEMBERS=10                                                    # Ensemble size\nE_WE=200                                                        # West-East grid points\nE_SN=240                                                        # South-North grid points\nE_BT=37                                                         # Vertical levels\n\n\nexport OMP_NUM_THREADS=1\nGSIPROC=10\n\nset-x\nIn principle, you only need to change BASEDIR and GSIDIR variables that are the path to the tutorial folder and the path to where GSI is compiled (the code expects to find a build folder with the executable files.\nSo, with that, you can run the script or send it to a queue.\n\nPossible issues\nThe script assume many things, in particular, where the configuration files, observations and background files are located. If you chance the structure of the folders and files, make sure to do the same in the script.\nThe other very possible issue is machine dependent. GSI creates files with the information of the observations and background called pe\\*something. Those files are later concatenated in diag_&lt;type_of_obs&gt;* files. This process depends on listing all the types of observations with some regex magic:\nlistall=`ls pe* | cut -f2 -d\".\" | awk '{print substr($0, 0, length($0)-2)}' | sort | uniq `\n\n   for type in $listall; do\n      count=`ls pe*${type}_${loop}* | wc -l`\n      if [[ $count -gt 0 ]]; then\n         cat pe*${type}_${loop}* &gt; diag_${type}_${string}.${ANAL_TIME} # For binary diag files\n      fi\n   done\nI had to slightly change that first line every time I changed machines. So, if you don’t see a bunch of diag* files in the GSI folder after running the script this is probably the reason.\n\n\nDid it work?\nIf you get a exit 0 at the end, it probably means that everything went well. However, I recommend you check a few things to make sure everything went really well.\n\nCheck that all the diag* files are there. You will get 1 file per member and ensemble mean for each type of observation. If you don’t see any of these files, check the Issues section. If you are missing the files for 1 type of observation, that probably means that the bufr file with the observations was not read properly or that is missing in the folder. Check if the script is linking the correct file to the GSI folder.\nCheck the statistics for each type of observations. You will find this information in the fit_&lt;obs&gt;.&lt;date&gt; files. Each one may have different information or structure depending on the type of observation, but make sure to check the number of observations read and keep by the system. This information is also included in the stdout file, you can sear READ_* to find the section in the file.\n\nIf you got an error number instead and, if you are lucky, the error code may be described in gsimain.f90."
  },
  {
    "objectID": "content/gsi/05-tutorial.html#running-enkf",
    "href": "content/gsi/05-tutorial.html#running-enkf",
    "title": "GSI tutorial",
    "section": "Running ENKF",
    "text": "Running ENKF\nThe second step to run GSI with the Kalman Filter method is running the code that performs the analysis. GSI will take the information provided by the first step (the diag* files) an calculate the final analysis.\nSimilarly to the first step, the script it’s almost ready to run and you only need to change BASEDIR and GSIDIR variables.\n#PBS -N tutorial-enkf \n#PBS -m abe \n#PBS -l walltime=03:00:00 \n#PBS -l nodes=2:ppn=96 \n#PBS -j oe \n\nBASEDIR=/home/paola.corrales/datosmunin3/tutorial_gsi           # Path to the tutorial folder\nGSIDIR=/home/paola.corrales/datosmunin3/comGSIv3.7_EnKFv1.3     # Path to where the GSI/EnKF code is compiled\nFECHA_INI='11:00:00 2018-11-22'                                 # Init time (analisis time - $ANALISIS)\nANALISIS=3600                                                   # Assimilation cycle in seconds\nOBSWIN=1                                                        # Assimilation window in hours\nN_MEMBERS=10                                                    # Ensemble size\nE_WE=200                                                        # West-East grid points\nE_SN=240                                                        # South-North grid points\nE_BT=37                                                         # Vertical levels\n\nENKFPROC=20\nexport OMP_NUM_THREADS=1\n\nset -x\nThe script will look for the GSI folder to link the diag* files and copy the background files to modify them into the analysis.\n\nPossible issues\nThis script also assume where the configuration files, background and diag* are located. So, if something is not working, check first if all the files are being copied or linked correctly.\nIt also includes a line to list all the types of observation and it is machine dependent, so that another source of problems. You can always type the list of observations by hand but you will need to update that every time.\n\n\nDid it work?\nIf you get a exit 0 at the end, it probably means that everything went well. Other things you can check:\n\nstdout file: the main thing to check is the innovation statistics for the prior and posterior (search for “innovation”) and the statistics for satellite brightness temperature. It will tell you how many observations were assimilated a few more details to get a sense of the impact of the observations.\nCheck the difference between the analysis and the background files. This requires a little more work but it is important to check this difference for at least one of the ensemble members. You can also do it for the ensemble mean, but note that the GSI system does not calculate the analysis ensemble mean, you will need to do it independently."
  },
  {
    "objectID": "content/gsi/05-tutorial.html#running-gsi-using-the-fgat-method",
    "href": "content/gsi/05-tutorial.html#running-gsi-using-the-fgat-method",
    "title": "GSI tutorial",
    "section": "Running GSI using the FGAT method",
    "text": "Running GSI using the FGAT method\nThe tutorial is configured to use the FGAT (First Guess at Appropriate Time) methods, this means that will GSI attempt to read multiple time level backgrounds a show in the following diagram.\n\n\n\nFGAT method\n\n\nThere is no option in the namelist or configuration files to use the FGAT method. To “activate” this option GSI needs to find the appropiate files in appropriate folder. In this example we have 7 files in total for each member, 1 background file at the assimilation time plus 3 files every 10 minutes before and after the assimilation time.\nSo, GSI will expect to find files called wrf_inou1 to wrf_inou7. The fonder structure looks like this:\nGUESS/\n└── 20181122120000\n    ├── 00\n    │   ├── wrfarw.ensmean\n    │   ├── wrfarw.mem001\n    │   ├── wrfarw.mem002\n    │   ├── wrfarw.mem003\n    │   ├── wrfarw.mem004\n    │   ├── wrfarw.mem005\n    │   ├── wrfarw.mem006\n    │   ├── wrfarw.mem007\n    │   ├── wrfarw.mem008\n    │   ├── wrfarw.mem009\n    │   └── wrfarw.mem010\n    ├── 01\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 02\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 03\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 04\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 05\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 06\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 07\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 08\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    ├── 09\n    │   ├── wrf_inou1\n    │   ├── wrf_inou2\n    │   ├── wrf_inou3\n    │   ├── wrf_inou4\n    │   ├── wrf_inou5\n    │   ├── wrf_inou6\n    │   └── wrf_inou7\n    └── 10\n        ├── wrf_inou1\n        ├── wrf_inou2\n        ├── wrf_inou3\n        ├── wrf_inou4\n        ├── wrf_inou5\n        ├── wrf_inou6\n        └── wrf_inou7\nThe files in folders 01 to 10 are the ones used during the assimilation. The files in fonder 00 where used to calculate the ensemble mean and can be also used to run GSI without the FGAT option. For that it is necessary to uncomment line 102 in the run_gsi.sh script:\n# BK_FILE_mem=${BK_ROOT}/wrfarw.mem\nand change lines 600 and 601 from:\ncp ${BK_ROOT}/${ensmemid}/wrf_inou* .\nBK_FILE_ANA=wrf_inou4\nto:\ncp ${BK_ROOT}/00/wrfarw_mem0${ensmemid}.\nBK_FILE_ANA=${BK_FILE}\nThat way, GSI will ignore the other folder and only use 1 background file for each member.\nTo check if GSI is doing what’s suppose to, we need to check the stdout file created during the observation operator step.\nCONVERT_NETCDF_MASS:  problem with flnm1 = wrf_inou1, Status =        -1021\nMeans that GSI did not find all the background files and only used the one at the analysis time.\nInstead if you see something this:\nconvert wrf_inou1 to sigf01\n  iy,m,d,h,m,s=        2018          11          22          11          30           0\n  dh1  =            1\n rmse_var = SMOIS\n ndim1 =            3\n ordering = XYZ\n staggering =  N/A\n start_index =            1           1           1           0\n end_index =          199         239           4           0\nYou’ll know that GSI is using the FGAT method. The example here shows that GSI found the wrf_inout1 file that is renaming to sig01 and then, information related to the time and domain characteristics. This will be repeated for each background file. In this case the first file corresponds to th 11:30 UTC of November 2018 and after that there is 1 file every 10 minutes.\nThis period between background files (10 minutes) is defined by the user when saving the background files. Again there is no configuration option to do this. GSI relies on finding the files in the especified folder.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe run_gsi.sh and run_enkf.sh scripts mentioned in this tutorial are derived from the example scripts provided with the Community GSIV3.7 Online Tutorial."
  },
  {
    "objectID": "content/gsi/05-tutorial.html#footnotes",
    "href": "content/gsi/05-tutorial.html#footnotes",
    "title": "GSI tutorial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe current version of the GSI system compiled using gnu returns an error when the LETKF algorithm is used.↩︎"
  },
  {
    "objectID": "content/gsi/03-radiances.html",
    "href": "content/gsi/03-radiances.html",
    "title": "Assimilating Radiance Observations",
    "section": "",
    "text": "Assimilating radiance observations is more complicated than assimilating conventional observations as radiances are not state variables. We need a observation operator to transform the model variables to radiances. GSI uses the Community Radiative Transfer Model (CRTM, Liu et al. 2008) as an operator of the radiance observations that calculates the brightness temperature simulated by the model in order to compare it with the observations from satellite sensors."
  },
  {
    "objectID": "content/gsi/03-radiances.html#the-crtm-radiative-transfer-model",
    "href": "content/gsi/03-radiances.html#the-crtm-radiative-transfer-model",
    "title": "Assimilating Radiance Observations",
    "section": "The CRTM radiative transfer model",
    "text": "The CRTM radiative transfer model\nThe CRTM is a fast radiative transfer model that was jointly developed by the NOAA Center for Satellite Applications and Research and the Joint Center for Satellite Data Assimilation (JCSDA). It is a model widely used by the remote sensing community as it is open source and publicly available. In addition, it is used for satellite instrument calibration (Weng et al. 2013; Iacovazzi et al. 2020; Crews et al. 2021), and in turn to generate retrievals from satellite observations (Boukabara et al. 2011; H. Hu et al. 2019; H. Hu and Han 2021). It is also used as an operator of observations as part of the assimilation of satellite radiances (Tong et al. 2020; Barton et al. 2021).\nThe CRTM is capable of simulating microwave, infrared and visible radiances, under clear and cloudy sky conditions, using atmospheric profiles of pressure, temperature, humidity and other species such as ozone. Recently Cutraro, Galligani, and Skabar (2021) evaluated their takeoff in the region with good results in simulating GOES-16 observations.\nCRTM is a sensor-oriented radiative transfer model, i.e. it contains pre-calculated parameterizations and coefficient tables specifically for operational sensors. It includes modules that calculate thermal radiation from gaseous absorption, absorption and scattering by aerosols and clouds, and emission and reflection of radiation by the Earth’s surface. The inputs of CRTM include atmospheric state variables, e.g., temperature, water vapor, pressure, and ozone concentration in user-defined layers, and surface state variables and parameters, including emissivity, surface temperature, and wind.\nCRTM is capable to simulate satellite observations from the state of the atmosphere. This is necessary during the assimilation process but is also used to verify the accuracy and errors of radiance observations.\nThe necessary calculations to simulate observations has a very high computational cost as it requires transposing a high dimensional matrix and the minimization of a cost function. This \\(K^{*}\\) matrix is constructed from the partial derivatives of the radiances with respect to geophysical parameters. CRTM performs these calculations very quickly so it can be used in operational contexts.\nTo obtain fast results, CRTM applies certain simplifications and approximations when solving the radiative transfer equation. First, it assumes that the Earth’s atmosphere consists of plane-parallel and homogeneous layers in thermodynamic equilibrium and where three-dimensional and polarization effects can be ignored.\nIn the context of clear skies, it is also assumed that there is no scattering and only the absorption of gases in the atmosphere is considered. In cloudy skies, the scattering generated by clouds is included. In the latter case, the radiative transfer equation cannot be solved analytically and numerical solutions are used."
  },
  {
    "objectID": "content/gsi/03-radiances.html#specific-configuration",
    "href": "content/gsi/03-radiances.html#specific-configuration",
    "title": "Assimilating Radiance Observations",
    "section": "Specific configuration",
    "text": "Specific configuration\nThe vertical location of each radiance observation was estimated as the model level at which its weight function computed by CRTM was maximized. The weight function of each channel corresponds to the change in transmittance with height and its maximum describes the layer of the atmosphere from which the radiation captured by the channel was emitted. Multispectral sensors have good vertical coverage and are capable of capturing from the lower troposphere to the lower stratosphere. The channels chosen for assimilation and their associated errors were defined taking into account the configuration that GSI uses to generate GFS analyses and forecasts, the model cap chosen in this work (50 hPa) and the possible influence of the surface (Table @ref(tab:table-rad))."
  },
  {
    "objectID": "content/gsi/03-radiances.html#observation-errors-and-quality-control",
    "href": "content/gsi/03-radiances.html#observation-errors-and-quality-control",
    "title": "Assimilating Radiance Observations",
    "section": "Observation errors and quality control",
    "text": "Observation errors and quality control\nThe preprocessing and quality control of the data is an essential step in the assimilation of radiances and depends on each sensor and channel. This process includes spatial thinning, bias correction, and in clear-sky applications, the detection of cloudy sky observations.\n\nThinning\nDuring the thinning process the observations to be assimilated are chosen based on their distance to the model grid points, the quality of the observation (based on available data quality information) and the number of available channels (for the same pixel and sensor). The thinning algorithm determines the quality of each observation based on the available information about the channels and their known errors, the type of surface below each pixel (preferring observations over the sea to those over land or snow) and predictors that give information about the quality of the observations (M. Hu et al. 2018). By applying the thinning we avoid incorporating information from smaller scale processes than the model can not represent and to reduce the error correlation of the observations from the same sensor.\n\n\nBias correction\nAfter the thinning, a bias correction is applied. The bias correction methodology implemented in GSI depends on thermodynamic characteristics of the air and on the scan angle (Zhu et al. 2014). It is computed as a linear polynomial of N predictors \\(p_i(x)\\), with associated coefficients \\(beta_i\\). Therefore, the bias-corrected brightness temperature (\\(BT_{cb}\\)) can be obtained as:\n\\[\\mathrm{\\mathit{BT_{cb}} =\\mathit{ BT} + \\sum_{i = 0}^{N} \\beta_i p_i (x)}\\]\nThe polynomial has a constant bias correction term (\\(p_0 = 1\\)) while the remaining terms and their predictors are the cloud liquid water (CLW) content, the rate of change of temperature with pressure, the square of the rate of change of temperature with pressure, and the sensitivity to the surface emissivity to account for the difference between land and sea. The scan angle-dependent bias is modeled as a polynomial of 4\\(^\\circ\\) order (Zhu et al. 2014).\nIn the GSI system, the coefficients \\(beta_i\\) are trained using a variational estimation method that generates the \\(beta_i\\) that provides the best fit between the simulation and the observations. The EnKF step also calculate the coefficients for the assimilation.\nIt is important to evaluate the training of the coefficients and the performance of the bias correction. One way to train the coefficients according to (zhu2024?) is to run the assimilation cycles for a long period of time, updating the coefficients at each cycle. While is possible to start the training with coefficients equal to zero, using the coefficients the GFS generates can help to speed up the process.\nTo check if the coefficients are correctly trained we can analysed the evolution of the different coefficients for each sensor and channel with time. As an example, here we show the coefficients for AMSU-A on board NOAA-15. Following Zhu et al. (2014), we expected the coefficients to reach a stable range of values after a certain period of time, this is evident for channel 4, 5, 6 and 8 but we see a continuous variation in channels 7 and 9.\n\n\n\nBias correction coefficients as function of time (days) for the training and experiment period. Channels 4 to 9 of AMSU-A on NOAA-15.\n\n\nUsing the resulting coefficients from the training period it is also important to check the impact of the bias correction. An easy way to see this is to calculate the mean difference between the observations and the first-guess (OmB) before and after the correction of the bias for each sensor. In the next figure there is an evident improvement as the mean OmB after the BC is now centered around zero and its standard deviation is smaller. This indicate that the BC correction worked as expected.\n\n\n\nMean difference between observations and first-guess after and before the correction of the bias calculate over a 3 days period for each sensor.\n\n\nThe training of the coefficients requires a lot of computational resources and can be challenging for observations from polar satellites used in regional applications. The reason for this is that the observations are only available 1 or 2 times a day, making the training a slow process. It is important to check that GSI is not penalizing the coefficients when there are no observations available.\n\n\nCloud detection\nThe cloud pixel detection methodology depends on the wavelength of the observations. For microwave radiances, potentially cloud-contaminated observations are detected using scattering and Liquid Water Path (LWP) indices calculated from differences between different channels of each sensor (Weston et al. 2019; Zhu et al. 2016). For infrared channels, cloud contaminated observations are detected using the transmittance profile calculated by the CRTM model. In addition, GSI checks the difference between the observations and the simulated brightness temperature to detect cloudy pixels. A particular case is the ABI observations since the cloud mask (level 2 product) available at the same resolution as the observations is used. This cloud mask is generated by combining information from 8 channels of the ABI sensor from the spatial and temporal point of view.\n\n\nOther quality controls\nThe GSI quality control filters out those observations from channels close to the visible range over water surfaces with a zenith angle greater than 60\\(^{{circ}\\) to reject those observations that could be contaminated by reflection. For infrared and microwave observations it also performs an emissivity check to detect observations contaminated by surface effects. Finally, a gross check is applied, i.e. the difference between the observation and the observation simulated by the model is compared with a predefined threshold depending on the observation error to reject erroneous observations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This guide is a compilation of many tools scripts, pieces of code and routines that I used during my PhD. As the name says, everything goes around assimilating observations using the GSI system. I spent a lot of time working with the conventional and Radiance observations so you will find a comprehensive tutorial and documentation on how to assimilate these types of observations. This includes the processing of the observations, how to configure the system and all the quality control steps GSI performs during the assimilation process.\nSometimes you will find everything inside the website and sometimes you will need to go to a specific repository to get the code or the data associated with the the specific topic. And while I tried to be mindful during my PhD at commenting and documenting everything along the way I’m sure that there are gaps and missing pieces. If you find anything that can be improved please open an issue in the repository of this website and contribute to making this guide better.\nMany of these tools come from different sources. Sometimes someone else wrote the code and I adapted it to my specific needs, sometimes I wrote the code from scratch. So it’s important to read the license note in each section so you know who created that code and in which way you can use it."
  },
  {
    "objectID": "about.html#about-my-phd",
    "href": "about.html#about-my-phd",
    "title": "About",
    "section": "About my PhD",
    "text": "About my PhD\nThe main goal of may research was to applied data assimilation techniques to improve short-term forecasts of severe events in Argentina. In particular, the research focuses on data assimilation of observations from automatic stations and radiances from polar and geostationary satellites. You can read the final dissertation (in Spanish) here.\nI worked in the Centro de Investigaciones del Mar y la Atmósfera (CIMA), for that reason you may find mentions to Hydra or Yakaira, their HPC and a server I used a lot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DA-documentation",
    "section": "",
    "text": "This is a work in progress\n\n\n\nComeback later to see more or, if you want to contribute, open an issue in the associated repo.\nThis website compiles a series of tutorials, scripts and in general a comprehensive documentation around the GSI system V3.7 - EnKF V1.3. It focus on the use of GSI as a observation operator along with the LETKF version. It covers everything you need from how to deal with observations (in bufr format), how to configure the system to how read and interpret GSI outputs."
  },
  {
    "objectID": "index.html#a-note-about-using-this-material",
    "href": "index.html#a-note-about-using-this-material",
    "title": "DA-documentation",
    "section": "A note about using this material",
    "text": "A note about using this material\nWhile the text in this guide is released under a Creative Commons Attribution-ShareAlike 4.0 International License, specific scripts or programming routines may have different licences and authors. If you use the material in any way, make sure to check the Licence note associated to each section and cite it appropriately."
  },
  {
    "objectID": "index.html#citing-this-guide",
    "href": "index.html#citing-this-guide",
    "title": "DA-documentation",
    "section": "Citing this guide",
    "text": "Citing this guide\nYou can cite this guide using its Zenodo metadata and DOI ."
  },
  {
    "objectID": "index.html#do-you-want-to-contribute",
    "href": "index.html#do-you-want-to-contribute",
    "title": "DA-documentation",
    "section": "Do you want to contribute?",
    "text": "Do you want to contribute?\nAny contribution is welcome, please read this guide to learn how to do it."
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "Contributors to this guide will appear here."
  },
  {
    "objectID": "content/observations/01-bufr.html",
    "href": "content/observations/01-bufr.html",
    "title": "Working with bufr files",
    "section": "",
    "text": "Intro\nTablas"
  },
  {
    "objectID": "content/gsi/01-gsi.html",
    "href": "content/gsi/01-gsi.html",
    "title": "The GSI assimilation system",
    "section": "",
    "text": "The GSI (Gridpoint Statistical Interpolation) System, is a state-of-the-art data assimilation system initially developed by the Environmental Modeling Center at NCEP. It was designed as a traditional 3DVAR system applied in the gridpoint space of models to facilitate the implementation of inhomogeneous anisotropic covariances (Wu, Purser, and Parrish 2002; Purser et al. 2003b, 2003a). It is designed to run on various computational platforms, create analyses for different numerical forecast models, and remain flexible enough to handle future scientific developments, such as the use of new observation types, improved data selection, and new state variables (Kleist et al. 2009).\nThe- 3DVAR system replaced the NCEP regional grid-point operational analysis system by the North American Mesoscale Prediction System (NAM) in 2006 and the Spectral Statistical Interpolation (SSI) global analysis system used to generate Global Forecast System (GFS) initial conditions in 2007 (Kleist et al. 2009). In recent years, GSI has evolved to include various data assimilation techniques for multiple operational applications, including 2DVAR [e.g., the Real-Time Mesoscale Analysis (RTMA) system; Pondeca et al. (2011)], the hybrid EnVar technique (e.g., data assimilation systems for the GFS, the Rapid Refresh system (RAP), the NAM, the HWRF, etc. ), and 4DVAR [e.g., the data assimilation system for NASA’s Goddard Earth Observing System, version 5 (GEOS-5); Zhu and Gelaro (2008)]. GSI also includes a hybrid 4D-EnVar approach that is currently used for GFS generation.\nIn addition to the development of hybrid techniques, GSI allows the use of ensemble assimilation methods. To achieve this, it uses the same observation operator as the variational methods to compare the preliminary field or background with the observations. In this way the exhaustive quality controls developed for variational methods are also applied in ensemble assimilation methods. The EnKF code was developed by the Earth System Research Lab (ESRL) of the National Oceanic and Atmospheric Administration (NOAA) in collaboration with the scientific community. It contains two different algorithms for calculating the analysis increment, the serial Ensemble Square Root Filter (EnSRF, Whitaker and Hamill 2002) and the LETKF (Hunt, Kostelich, and Szunyogh 2007) contributed by Yoichiro Ota of the Japan Meteorological Agency (JMA).\nTo reduce the impact of spurious covariances on the increment applied to the analysis, ensemble systems apply a localization to the covariance matrix of the errors of the observations \\(R\\) in both the horizontal and vertical directions. GSI uses a polynomial of order 5 to reduce the impact of each observation gradually until a limiting distance is reached at which the impact is zero. The vertical location scale is defined in terms of the logarithm of the pressure and the horizontal scale is usually defined in kilometers. These parameters are important in obtaining a good analysis and depend on factors such as the size of the ensemble and the resolution of the model.\nGSI uses the Community Radiative Transfer Model (CRTM, Liu et al. 2008) as an operator for the radiance observations that calculates the brightness temperature simulated by the model in order to compare it with satellite sensor observations. GSI also implements a bias correction algorithm for the satellite radiance observations. The preliminary field estimate with the CRMT is compared with the radiance observations to obtain the innovation. This innovation is then used to calculate a bias that is applied to an updated innovation. This process can be repeated several times until the innovation and the bias correction coefficients converge."
  },
  {
    "objectID": "content/gsi/01-gsi.html#available-observations-for-assimilation",
    "href": "content/gsi/01-gsi.html#available-observations-for-assimilation",
    "title": "The GSI assimilation system",
    "section": "Available observations for assimilation",
    "text": "Available observations for assimilation\nHere is the list of observations that can be assimilated by GSI. In bold are the observations for with I have experience and/or the ones I’ve adapted the code for it.\n\nConventional observations (including satellite retrievals):\n\nRadiosondes\nPilot ballon (PIBAL) winds\nSynthetic tropical cyclone winds\nWind profilers: USA, Jan Meteorological Agency (JMA)\nConventional aircraft reports\nAircraft to Satellite Data Relay (ASDAR) aircraft reports\nMeteorological Data Collection and Reporting System (MDCRS) aircraft reports\nDropsondes\nModerate Resolution Imaging Spectroradiometer (MODIS) IR and water vapor winds\nGeostationary Meteorological Satellite (GMS), JMA, and Meteosat cloud drift IR and visible winds\nEuropean Organization for the Exploitation of Meteorological Satellites (EUMETSAT) and GOES water vapor cloud top winds\nGEOS hourly IR and cloud top wind\nSurface land observations\nSurface ship and buoy observations\nSpecial Sensor Microwave Imager (SSMI) wind speeds\nQuick Scatterometer (QuikSCAT), the Advanced Scatterometer (ASCAT) and Oceansat-2 Scatterometer (OSCAT) wind speed and direction\nRapidScat observations\nSSM/I and Tropical Rainfall Measuring Mission (TRMM) Microwave Imager (TMI) precipitation estimates\nVelocity-Azimuth Display (VAD) Next Generation Weather Radar ((NEXRAD) winds\nGlobal Positioning System (GPS) precipitable water estimates Sea surface temperatures (SSTs)\nDoppler wind Lidar\nAviation routine weather report (METAR) cloud coverage\nFlight level and Stepped Frequency Microwave Radiometer (SFMR) High Density Observation (HDOB) from reconnaissance aircraft\nTall tower wind\n\n\n\nSatellite radiance/brightness temperature observations (instrument/satellite ID):\n\nSBUV: NOAA-17, NOAA-18, NOAA-19\nHigh Resolution Infrared Radiation Sounder (HIRS): Meteorological Operational-A(MetOp-A), MetOp-B, NOAA-17, NOAA-19\nGOES imager: GOES-11, GOES-12\nAtmospheric IR Sounder (AIRS): aqua\nAMSU-A: MetOp-A, MetOp-B, NOAA-15, NOAA-18, NOAA-19, aqua\nAMSU-B: MetOp-B, NOAA-17\nMicrowave Humidity Sounder (MHS): MetOp-A, MetOp-B, NOAA-18, NOAA-19\nSSMI: DMSP F14, F15, F19\nSSMI/S: DMSP F16\nAdvanced Microwave Scanning Radiometer for Earth Observing System (AMSR-E): aqua\nGOES Sounder (SNDR): GOES-11, GOES-12, GOES-13\nInfrared Atmospheric Sounding Interferometer (IASI): MetOp-A, MetOp-B\nGlobal Ozone Monitoring Experiment (GOME): MetOp-A, MetOp-B\nOzone Monitoring Instrument (OMI): aura\nSpinning Enhanced Visible and Infrared Imager (SEVIRI): Meteosat-8, Meteosat-9, Meteosat-10\nAdvanced Technology Microwave Sounder (ATMS): Suomi NPP\nCross-track Infrared Sounder (CrIS): Suomi NPP\nGCOM-W1 AMSR2\nGPM GMI\nMegha-Tropiques SAPHIR\nHimawari AHI\nGOES ABI\n\n\n\nOthers:\n\nGPS Radio occultation (RO) refractivity and bending angle profiles\nSolar Backscatter Ultraviolet (SBUV) ozone profiles, Microwave Limb Sounder (MLS) (including NRT) ozone, * and Ozone Monitoring Instrument (OMI) total ozone\nDoppler radar radial velocities radar reflectivity Mosaic\nTail Doppler Radar (TDR) radial velocity and super-observation\nTropical Cyclone Vitals Database (TCVital)\nParticulate matter (PM) of 10-um diameter, 2.5-um diameter or less\nMODIS AOD (when using GSI-chem package)\nSignificant wave height observations from JASON-2, JASON-3, SARAL/ALTIKA and CRYOSAT-2"
  },
  {
    "objectID": "content/gsi/01-gsi.html#running-gsi",
    "href": "content/gsi/01-gsi.html#running-gsi",
    "title": "The GSI assimilation system",
    "section": "Running GSI",
    "text": "Running GSI\nEvery assimilation cycle starts with the background, a forecast generated using a numerical model (WRF-ARW for this guide), that was initialized from previous analysis and observations (in bufr format) that enters the GSI system. GSI will also need “fixed” files with information about the observations. This files define which observations are going to be assimilated, they errors and quality control options.\n\n\n\nDiagram of an assimilation cycle\n\n\nGSI can also be used with the following background files:\n\nWRF-NMM input fields in binary format\nWRF-NMM input fields in NetCDF format\nWRF-ARW input fields in binary format\nWRF-ARW input fields in NetCDF format\nGFS input fields in binary format or through NEMS I/O\nNEMS-NMMB input fields\nRTMA input files (2-dimensional binary format)\nWRF-Chem GOCART input fields with NetCDF format\nCMAQ binary file\n\nAnd the official tutorials are a good starting point to grasp the use of this options.\nGSI can also be run without observations to test the code, this is with a single synthetic observation defined in the SINGLEOB_TEST section in the namelist.\nThe fixed files are located in the fix/ folder and includes statistic files, configuration files, bias correction files, and CRTM coefficient files1. The information of the configuration files is saved in the output files after running GSI.\n\n\n\n\n\n\n\n\nGSI Name\nContent\nFile names\n\n\n\n\nanavinfo\nInformation file to set control and analysis variables\nanavinfo_arw_netcdf\n\n\nberror_stats\nbackground error covariance (for variacional methods)\nnam_nmmstat_na.gcv, nam_glb_berror.f77.gcv,\n\n\nerrtable\nObservation error table\nprepobs_errtable.global\n\n\nconvinfo\nConventional observation information file\nglobal_convinfo.txt\n\n\nsatinfo\nsatellite channel information file\nglobal_satinfo.txt\n\n\npcpinfo\nprecipitation rate observation information file\nglobal_pcpinfo.txt\n\n\nozinfo\nozone observation information file\nglobal_ozinfo.txt\n\n\nsatbias_angle\nsatellite scan angle dependent bias correction file\nglobal_satangbias.txt\n\n\nsatbias_in\nsatellite mass bias correction coefficient file\nsample.satbias\n\n\nsatbias_in\ncombined satellite angle dependent and mass bias correction coefficient file\ngdas1.t00z.abias.new"
  },
  {
    "objectID": "content/gsi/01-gsi.html#footnotes",
    "href": "content/gsi/01-gsi.html#footnotes",
    "title": "The GSI assimilation system",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis files need to be downloaded separately as they are to big to be part of the GSI repository. Also the coefficient files can be updated with better approximations over time.↩︎"
  },
  {
    "objectID": "content/gsi/04-diagfiles.html",
    "href": "content/gsi/04-diagfiles.html",
    "title": "Undestanding diag files",
    "section": "",
    "text": "Scrips para decodificar\nrutina que los genera"
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "How to contribute",
    "section": "",
    "text": "This outlines how to propose a change to DA-documentation."
  },
  {
    "objectID": "contribute.html#fixing-typos",
    "href": "contribute.html#fixing-typos",
    "title": "How to contribute",
    "section": "Fixing typos",
    "text": "Fixing typos\nYou can fix typos, spelling mistakes, or grammatical errors in the documentation directly using the GitHub web interface, as long as the changes are made in the source file. This will automatically open a pull request"
  },
  {
    "objectID": "contribute.html#bigger-changes",
    "href": "contribute.html#bigger-changes",
    "title": "How to contribute",
    "section": "Bigger changes",
    "text": "Bigger changes\nIf you want to make a bigger change, it’s a good idea to first file an issue to start a conversation."
  },
  {
    "objectID": "contribute.html#pull-request-process",
    "href": "contribute.html#pull-request-process",
    "title": "How to contribute",
    "section": "Pull request process",
    "text": "Pull request process\nTo open a pull request you’ll need to fork the repository. This website is builded with quarto, make sure you have it installed if you want to build the website locally to test the chances you are proposing."
  },
  {
    "objectID": "contribute.html#acknowledgements",
    "href": "contribute.html#acknowledgements",
    "title": "How to contribute",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nIf you contribute to this guide, I’d like to publicly acknowledge you work. Please indicate in you PR if you want to be add to the Contributors page."
  }
]